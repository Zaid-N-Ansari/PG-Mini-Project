{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56f55d46",
   "metadata": {},
   "source": [
    "##   üè° Concrete Strength Prediction: From Exploration to Modeling üèóÔ∏è\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "##   1\\. üßê Project Initialization: Library Imports\n",
    "\n",
    "We begin by importing the necessary Python libraries. These tools will help us handle data, perform calculations, visualize information, build machine learning models, and evaluate their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361447fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import log1p\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf265c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "##   2\\. üì• Data Loading and Inspection\n",
    "\n",
    "In this step, we load the concrete strength dataset from a CSV file. We then identify the features (input variables) and the target variable ('strength', which is what we want to predict). Finally, we display the first few rows of the data to get an initial look at its contents.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe4d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>ash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplastic</th>\n",
       "      <th>coarseagg</th>\n",
       "      <th>fineagg</th>\n",
       "      <th>age</th>\n",
       "      <th>strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141.3</td>\n",
       "      <td>212.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>203.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>971.8</td>\n",
       "      <td>748.5</td>\n",
       "      <td>28</td>\n",
       "      <td>29.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168.9</td>\n",
       "      <td>42.2</td>\n",
       "      <td>124.3</td>\n",
       "      <td>158.3</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1080.8</td>\n",
       "      <td>796.2</td>\n",
       "      <td>14</td>\n",
       "      <td>23.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.7</td>\n",
       "      <td>187.4</td>\n",
       "      <td>5.5</td>\n",
       "      <td>956.9</td>\n",
       "      <td>861.2</td>\n",
       "      <td>28</td>\n",
       "      <td>29.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>266.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>28</td>\n",
       "      <td>45.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154.8</td>\n",
       "      <td>183.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>193.3</td>\n",
       "      <td>9.1</td>\n",
       "      <td>1047.4</td>\n",
       "      <td>696.7</td>\n",
       "      <td>28</td>\n",
       "      <td>18.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>961.0</td>\n",
       "      <td>805.0</td>\n",
       "      <td>28</td>\n",
       "      <td>13.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>531.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.8</td>\n",
       "      <td>28.2</td>\n",
       "      <td>852.1</td>\n",
       "      <td>893.7</td>\n",
       "      <td>3</td>\n",
       "      <td>41.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>276.4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>90.3</td>\n",
       "      <td>179.6</td>\n",
       "      <td>8.9</td>\n",
       "      <td>870.1</td>\n",
       "      <td>768.3</td>\n",
       "      <td>28</td>\n",
       "      <td>44.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>342.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>270</td>\n",
       "      <td>55.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>613.0</td>\n",
       "      <td>7</td>\n",
       "      <td>52.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1030 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cement   slag    ash  water  superplastic  coarseagg  fineagg  age  \\\n",
       "0      141.3  212.0    0.0  203.5           0.0      971.8    748.5   28   \n",
       "1      168.9   42.2  124.3  158.3          10.8     1080.8    796.2   14   \n",
       "2      250.0    0.0   95.7  187.4           5.5      956.9    861.2   28   \n",
       "3      266.0  114.0    0.0  228.0           0.0      932.0    670.0   28   \n",
       "4      154.8  183.4    0.0  193.3           9.1     1047.4    696.7   28   \n",
       "...      ...    ...    ...    ...           ...        ...      ...  ...   \n",
       "1025   135.0    0.0  166.0  180.0          10.0      961.0    805.0   28   \n",
       "1026   531.3    0.0    0.0  141.8          28.2      852.1    893.7    3   \n",
       "1027   276.4  116.0   90.3  179.6           8.9      870.1    768.3   28   \n",
       "1028   342.0   38.0    0.0  228.0           0.0      932.0    670.0  270   \n",
       "1029   540.0    0.0    0.0  173.0           0.0     1125.0    613.0    7   \n",
       "\n",
       "      strength  \n",
       "0        29.89  \n",
       "1        23.51  \n",
       "2        29.22  \n",
       "3        45.85  \n",
       "4        18.29  \n",
       "...        ...  \n",
       "1025     13.29  \n",
       "1026     41.30  \n",
       "1027     44.28  \n",
       "1028     55.06  \n",
       "1029     52.61  \n",
       "\n",
       "[1030 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv('https://raw.githubusercontent.com/rahulinchal/SPPU/refs/heads/main/Data/concrete_Data.csv')\n",
    "df = pd.read_csv('./Data/Concrete.csv')\n",
    "features = df.columns[:-1].tolist()\n",
    "target = 'strength'\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa6674c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "##   3\\. üìä Initial Data Exploration: Box Plots\n",
    "\n",
    "The following code was designed to create box plots. Box plots are a useful way to visualize the distribution of data and identify potential outliers (extreme values).\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f7cdb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(25, 20))\n",
    "# plt.subplot(3, 3, 1)\n",
    "# plt.title('Box plot of features')\n",
    "# plt.xlabel('Features')\n",
    "# plt.ylabel('Value')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.grid(axis='y')\n",
    "# plt.boxplot([df[feature] for feature in features], labels=features, showfliers=True)\n",
    "# plt.show()\n",
    "\n",
    "# for feature in features:\n",
    "# \tplt.figure(figsize=(10, 5))\n",
    "# \tplt.boxplot(df[feature], showfliers=True, vert=False)\n",
    "# \tplt.title(f'Box plot of {feature}')\n",
    "# \tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d55a058d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>ash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplastic</th>\n",
       "      <th>coarseagg</th>\n",
       "      <th>fineagg</th>\n",
       "      <th>age</th>\n",
       "      <th>strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141.3</td>\n",
       "      <td>212.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>203.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>971.8</td>\n",
       "      <td>748.5</td>\n",
       "      <td>28</td>\n",
       "      <td>29.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168.9</td>\n",
       "      <td>42.2</td>\n",
       "      <td>124.3</td>\n",
       "      <td>158.3</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1080.8</td>\n",
       "      <td>796.2</td>\n",
       "      <td>14</td>\n",
       "      <td>23.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.7</td>\n",
       "      <td>187.4</td>\n",
       "      <td>5.5</td>\n",
       "      <td>956.9</td>\n",
       "      <td>861.2</td>\n",
       "      <td>28</td>\n",
       "      <td>29.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>266.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>28</td>\n",
       "      <td>45.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154.8</td>\n",
       "      <td>183.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>193.3</td>\n",
       "      <td>9.1</td>\n",
       "      <td>1047.4</td>\n",
       "      <td>696.7</td>\n",
       "      <td>28</td>\n",
       "      <td>18.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>961.0</td>\n",
       "      <td>805.0</td>\n",
       "      <td>28</td>\n",
       "      <td>13.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>531.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.8</td>\n",
       "      <td>28.2</td>\n",
       "      <td>852.1</td>\n",
       "      <td>893.7</td>\n",
       "      <td>3</td>\n",
       "      <td>41.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>276.4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>90.3</td>\n",
       "      <td>179.6</td>\n",
       "      <td>8.9</td>\n",
       "      <td>870.1</td>\n",
       "      <td>768.3</td>\n",
       "      <td>28</td>\n",
       "      <td>44.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>342.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>270</td>\n",
       "      <td>55.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>613.0</td>\n",
       "      <td>7</td>\n",
       "      <td>52.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>998 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cement   slag    ash  water  superplastic  coarseagg  fineagg  age  \\\n",
       "0      141.3  212.0    0.0  203.5           0.0      971.8    748.5   28   \n",
       "1      168.9   42.2  124.3  158.3          10.8     1080.8    796.2   14   \n",
       "2      250.0    0.0   95.7  187.4           5.5      956.9    861.2   28   \n",
       "3      266.0  114.0    0.0  228.0           0.0      932.0    670.0   28   \n",
       "4      154.8  183.4    0.0  193.3           9.1     1047.4    696.7   28   \n",
       "...      ...    ...    ...    ...           ...        ...      ...  ...   \n",
       "1025   135.0    0.0  166.0  180.0          10.0      961.0    805.0   28   \n",
       "1026   531.3    0.0    0.0  141.8          28.2      852.1    893.7    3   \n",
       "1027   276.4  116.0   90.3  179.6           8.9      870.1    768.3   28   \n",
       "1028   342.0   38.0    0.0  228.0           0.0      932.0    670.0  270   \n",
       "1029   540.0    0.0    0.0  173.0           0.0     1125.0    613.0    7   \n",
       "\n",
       "      strength  \n",
       "0        29.89  \n",
       "1        23.51  \n",
       "2        29.22  \n",
       "3        45.85  \n",
       "4        18.29  \n",
       "...        ...  \n",
       "1025     13.29  \n",
       "1026     41.30  \n",
       "1027     44.28  \n",
       "1028     55.06  \n",
       "1029     52.61  \n",
       "\n",
       "[998 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eise hi uncomment karna outlier treatment ke liye, sirf eise hi aur konsa bhi outliers treatment wala nhi\n",
    "    #    ^\n",
    "    #    |\n",
    "    #    |\n",
    "    #    |\n",
    "    #    |\n",
    "df = df[df['water'] < 240]\n",
    "df = df[df['superplastic'] < 30]\n",
    "df = df[df['fineagg'] < 960]\n",
    "df = df[df['age'] < 300]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c379be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "##   4\\. üìä Post-Outlier Removal Visualization\n",
    "\n",
    "This code would generate box plots *after* the outlier removal in the previous step. Comparing these plots to those in section 3 would show the impact of removing outliers.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "746f959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(25, 20))\n",
    "# plt.subplot(3, 3, 1)\n",
    "# plt.title('Box plot of features')\n",
    "# plt.xlabel('Features')\n",
    "# plt.ylabel('Value')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.grid(axis='y')\n",
    "# plt.boxplot([df[feature] for feature in features], labels=features, showfliers=True)\n",
    "# plt.show()\n",
    "\n",
    "# for feature in features:\n",
    "# \tplt.figure(figsize=(10, 5))\n",
    "# \tplt.boxplot(df[feature], showfliers=True, vert=False)\n",
    "# \tplt.title(f'Box plot of {feature}')\n",
    "# \tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165f6268",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "##   5\\. üõ†Ô∏è Feature Transformation and Data Splitting\n",
    "\n",
    "This is a critical data preprocessing stage. First, we address skewness (asymmetry) in the data by applying a log transformation to features that exhibit high skewness.  Then, we split the data into training and testing sets. The training set is used to train the models, and the testing set is used to evaluate their performance on unseen data. Finally, we scale the numerical features using `StandardScaler`. Scaling ensures that all features contribute equally to the models.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8bf847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "highly_skewed_features = df[features].skew()[abs(df[features].skew()) > 0.5].index\n",
    "\n",
    "for feature in highly_skewed_features:\n",
    "    df[feature] = log1p(df[feature])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d001c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "##   6\\. üìä Post-Transformation Visualization (Commented Out)\n",
    "\n",
    "This commented-out section would visualize the feature distributions *after* the log transformation, allowing us to see how the transformation affected the skewness.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bf04c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(25, 20))\n",
    "# plt.subplot(3, 3, 1)\n",
    "# plt.title('Box plot of features')\n",
    "# plt.xlabel('Features')\n",
    "# plt.ylabel('Value')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.grid(axis='y')\n",
    "# plt.boxplot([df[feature] for feature in features], labels=features, showfliers=True)\n",
    "# plt.show()\n",
    "\n",
    "# for feature in features:\n",
    "# \tplt.figure(figsize=(10, 5))\n",
    "# \tplt.boxplot(df[feature], showfliers=True, vert=False)\n",
    "# \tplt.title(f'Box plot of {feature}')\n",
    "# \tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1ebbbb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "##   8\\. ü§ñ Linear Regression Model\n",
    "\n",
    "Here, we train a Linear Regression model. We then evaluate its performance using the R-squared metric, which measures how well the model fits the data. We calculate the R-squared on both the training and testing sets to check for overfitting (when the model performs much better on the training data than on the testing data).\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c36b40e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.32785659503517 81.82655042523399 0.4986938301988175 0.5013061698011825\n"
     ]
    }
   ],
   "source": [
    "LR = LinearRegression()\n",
    "LR.fit(x_train, y_train)\n",
    "y_pred = LR.predict(x_test)\n",
    "y_train_pred = LR.predict(x_train)\n",
    "training_r2 = LR.score(x_train, y_train) * 100\n",
    "testing_r2 = LR.score(x_test, y_test) * 100\n",
    "diff = abs(training_r2 - testing_r2)\n",
    "print(training_r2, testing_r2, diff, 1-diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6741cd57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "##   9\\. üå≥ Decision Tree Regressor\n",
    "\n",
    "This section trains and evaluates a Decision Tree Regressor. <br>\n",
    "Hyperparameters (parameters that control the model's structure) such as `ccp_alpha`, `max_depth`, `max_leaf_nodes`, and `min_samples_split` are set to influence the tree's complexity.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c97998f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.42477563661568 73.78008052318323 8.644695113432448 -7.644695113432448         cement      slag    ash  water  superplastic  coarseagg  fineagg  \\\n",
      "0     4.957938  5.361292    0.0  203.5      0.000000      971.8    748.5   \n",
      "1     5.135210  3.765840  124.3  158.3      2.468100     1080.8    796.2   \n",
      "2     5.525453  0.000000   95.7  187.4      1.871802      956.9    861.2   \n",
      "3     5.587249  4.744932    0.0  228.0      0.000000      932.0    670.0   \n",
      "4     5.048573  5.217107    0.0  193.3      2.312535     1047.4    696.7   \n",
      "...        ...       ...    ...    ...           ...        ...      ...   \n",
      "1025  4.912655  0.000000  166.0  180.0      2.397895      961.0    805.0   \n",
      "1026  6.277207  0.000000    0.0  141.8      3.374169      852.1    893.7   \n",
      "1027  5.625461  4.762174   90.3  179.6      2.292535      870.1    768.3   \n",
      "1028  5.837730  3.663562    0.0  228.0      0.000000      932.0    670.0   \n",
      "1029  6.293419  0.000000    0.0  173.0      0.000000     1125.0    613.0   \n",
      "\n",
      "           age  strength  \n",
      "0     3.367296     29.89  \n",
      "1     2.708050     23.51  \n",
      "2     3.367296     29.22  \n",
      "3     3.367296     45.85  \n",
      "4     3.367296     18.29  \n",
      "...        ...       ...  \n",
      "1025  3.367296     13.29  \n",
      "1026  1.386294     41.30  \n",
      "1027  3.367296     44.28  \n",
      "1028  5.602119     55.06  \n",
      "1029  2.079442     52.61  \n",
      "\n",
      "[998 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "DTR = DecisionTreeRegressor(ccp_alpha=0.689, max_depth=80, max_leaf_nodes=87, min_samples_split=26, max_features=4)\n",
    "DTR.fit(x_train, y_train)\n",
    "y_pred = DTR.predict(x_test)\n",
    "y_train_pred = DTR.predict(x_train)\n",
    "\n",
    "training_r2 = DTR.score(x_train, y_train) * 100\n",
    "testing_r2 = DTR.score(x_test, y_test) * 100\n",
    "diff = abs(training_r2 - testing_r2)\n",
    "print(f'{training_r2} {testing_r2} {diff} {1-diff} {_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4886df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "##   10\\. üå≤ Random Forest and XGBoost Regressors\n",
    "\n",
    "This part explores more advanced models: Random Forest and XGBoost. <br>\n",
    "These are ensemble methods, which combine multiple simpler models to make more accurate predictions. Hyperparameters are again used to tune the models.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28893e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.00463691522623 85.20503966832244 2.7995972469037866 -1.7995972469037866\n"
     ]
    }
   ],
   "source": [
    "RFR = RandomForestRegressor(n_estimators=825, min_samples_split=8, min_samples_leaf=8, max_features=0.49, bootstrap=True, oob_score=True)\n",
    "RFR.fit(x_train, y_train)\n",
    "y_pred = RFR.predict(x_test)\n",
    "y_train_pred = RFR.predict(x_train)\n",
    "\n",
    "training_r2 = RFR.score(x_train, y_train) * 100\n",
    "testing_r2 = RFR.score(x_test, y_test) * 100\n",
    "diff = abs(training_r2 - testing_r2)\n",
    "print(training_r2, testing_r2, diff, 1-diff)\n",
    "\n",
    "# XGB = XGBRegressor(n_estimators=354, learning_rate=0.009, max_depth=6, gamma=0.15, subsample=0.045, colsample_bytree=0.8)\n",
    "# XGB.fit(x_train, y_train)\n",
    "\n",
    "# y_pred = XGB.predict(x_test)\n",
    "# y_train_pred = XGB.predict(x_train)\n",
    "# training_r2 = XGB.score(x_train, y_train) * 100\n",
    "# testing_r2 = XGB.score(x_test, y_test) * 100\n",
    "# diff = abs(training_r2 - testing_r2)\n",
    "# print(training_r2, testing_r2, diff, 1-diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2e35b",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500, 800, 1200],# Number of trees\n",
    "    'max_depth': [None, 5, 10, 15],# Maximum depth of the trees\n",
    "    'min_samples_split': [2, 5, 10, 20],# Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 5, 10, 20], # Minimum number of samples required at a leaf node\n",
    "    'min_impurity_decrease': [0.0, 0.05, 0.1], # Minimum impurity decrease for a split\n",
    "    'max_features': ['sqrt', 'log2', 0.6, 0.8], # Number of features to consider at each split\n",
    "    'bootstrap': [True, False]# Whether to use bootstrap samples\n",
    "}\n",
    "\n",
    "RFR = RandomForestRegressor(oob_score=True, random_state=42) # Added random_state for reproducibility\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RFR,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,# Number of cross-validation folds\n",
    "    scoring='r2',# Metric to optimize (R-squared)\n",
    "    n_jobs=-1,# Use all available cores\n",
    "    verbose=2\n",
    ")# Display progress\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best R-squared score:\", grid_search.best_score_)\n",
    "best_rfr = grid_search.best_estimator_\n",
    "y_pred_tuned = best_rfr.predict(x_test)\n",
    "y_train_pred_tuned = best_rfr.predict(x_train)\n",
    "training_r2_tuned = best_rfr.score(x_train, y_train) * 100\n",
    "testing_r2_tuned = best_rfr.score(x_test, y_test) * 100\n",
    "diff_tuned = abs(training_r2_tuned - testing_r2_tuned)\n",
    "print(\"Tuned Training R2:\", training_r2_tuned)\n",
    "print(\"Tuned Testing R2:\", testing_r2_tuned)\n",
    "print(\"Tuned Difference:\", diff_tuned)\n",
    "print(\"Tuned (1 - Difference):\", 1 - diff_tuned)\n",
    "```\n",
    "### OUTPUT:\n",
    "Fitting 3 folds for each of 7680 candidates, totalling 23040 fits\n",
    "\n",
    "e:\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
    "11520 fits failed out of a total of 23040.\n",
    "The score on these train-test partitions for these parameters will be set to nan.\n",
    "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
    "\n",
    "Below are more details about the failures:\n",
    "--------------------------------------------------------------------------------\n",
    "11520 fits failed with the following error:\n",
    "Traceback (most recent call last):\n",
    "  File \"e:\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
    "    estimator.fit(X_train, y_train, **fit_params)\n",
    "  File \"e:\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
    "    return fit_method(estimator, *args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"e:\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 417, in fit\n",
    "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
    "ValueError: Out of bag estimation only available if bootstrap=True\n",
    "\n",
    "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
    "e:\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [0.84728255 0.85030631 0.85021671 ...        nan        nan        nan]\n",
    "\n",
    "Best hyperparameters: {'bootstrap': True, 'max_depth': None, 'max_features': 0.6, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 800}\n",
    "Best R-squared score: 0.86545690159861\n",
    "Tuned Training R2: 98.22626431289588\n",
    "Tuned Testing R2: 93.0819510242481\n",
    "Tuned Difference: 5.14431328864778\n",
    "Tuned (1 - Difference): -4.14431328864778"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7fe83",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "##   12\\. ‚öôÔ∏è Support Vector Regression (SVR)\n",
    "\n",
    "This part demonstrates training and evaluating a Support Vector Regression (SVR) model. SVR is another powerful regression technique. <br>\n",
    "Hyperparameters like `C`, `gamma`, and `epsilon` are tuned to control the model's behavior.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b56a368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.21301362638955 87.68199685877651 0.5310167676130391 0.46898323238696094\n"
     ]
    }
   ],
   "source": [
    "SVM = SVR(C=410, gamma=0.01, epsilon=0.06)\n",
    "SVM.fit(x_train, y_train)\n",
    "y_pred = SVM.predict(x_test)\n",
    "y_train_pred = SVM.predict(x_train)\n",
    "\n",
    "training_r2 = SVM.score(x_train, y_train) * 100\n",
    "testing_r2 = SVM.score(x_test, y_test) * 100\n",
    "diff = abs(training_r2 - testing_r2)\n",
    "print(training_r2, testing_r2, diff, 1-diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccb9d73",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "##   13\\. üëØ‚Äç‚ôÄÔ∏è K-Nearest Neighbors (KNN) and XGBoost (Repeated)\n",
    "\n",
    "<li>This final section repeats the training and evaluation of KNN and XGBoost.\n",
    "<li> It's possible that different hyperparameters are being explored here compared to earlier sections.\n",
    "<li> Comparing the results from different runs is essential for model selection.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a0c6e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.42361341220936 80.15828061920818 1.2653327930011784 -0.2653327930011784\n",
      "82.987611701891 82.71176954659256 0.2758421552984345 0.7241578447015655\n"
     ]
    }
   ],
   "source": [
    "KNN = KNeighborsRegressor(algorithm='auto', metric='minkowski', p=2, leaf_size=65, n_neighbors=15)\n",
    "\n",
    "KNN.fit(x_train, y_train)\n",
    "\n",
    "y_pred = KNN.predict(x_test)\n",
    "y_train_pred = KNN.predict(x_train)\n",
    "\n",
    "training_r2 = KNN.score(x_train, y_train) * 100\n",
    "testing_r2 = KNN.score(x_test, y_test) * 100\n",
    "\n",
    "diff = abs(training_r2 - testing_r2)\n",
    "\n",
    "print(training_r2, testing_r2, diff, 1-diff)\n",
    "\n",
    "XGB = XGBRegressor(n_estimators=354, learning_rate=0.0099, max_depth=6, gamma=0.12, subsample=0.045, colsample_bytree=0.8)\n",
    "XGB.fit(x_train, y_train)\n",
    "\n",
    "y_pred = XGB.predict(x_test)\n",
    "y_train_pred = XGB.predict(x_train)\n",
    "training_r2 = XGB.score(x_train, y_train) * 100\n",
    "testing_r2 = XGB.score(x_test, y_test) * 100\n",
    "diff = abs(training_r2 - testing_r2)\n",
    "print(training_r2, testing_r2, diff, 1-diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d275e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
